{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    # IMP : initialize the value function to zeros initially, check if the results change with different initialilzations - shouldn't change ideally\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    # the V array is not changed in the following iterations, this is imp! :P\n",
    "    for _ in range(100):\n",
    "        while True:\n",
    "        # TODO: Implement!\n",
    "            threshold = 0\n",
    "\n",
    "            for s in range(env.nS):\n",
    "                v = 0\n",
    "\n",
    "                # here a is just an index to the set of all possible actions from state s, see - http://book.pythontips.com/en/latest/enumerate.html\n",
    "                # we are going over all the possible actions from state s\n",
    "                # policy matrix contains the probability of an action being taken from a state\n",
    "                for a, action_prob in enumerate(policy[s]):\n",
    "                    # env.P[s][a] has the transition probabilities as given by the environment - here we are using the environment's knowledge in making decisions\n",
    "                    for prob, next_state, reward, info in env.P[s][a]:\n",
    "                        # the order of these values is important, don't change the order! - needs lots of debugging\n",
    "                        v += action_prob * prob *(reward + discount_factor * V[next_state])\n",
    "                        # bootstrapping - guessing V[next_state] and using it to make current calculation\n",
    "\n",
    "                threshold = max(threshold, v - V[s])\n",
    "                V[s] = v\n",
    "            # stop if the maximum change in value function is less than some threshold\n",
    "            if threshold < theta:\n",
    "                break\n",
    "            \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Make sure the evaluated policy is what we expected\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**IMP:** To ensure that we hit optimal value function, run the while loop of policy_eval function about 100 times, essentially, we keep on iterating until hitting the optimal value function. But most of the times, value function converges much more faster than this, because we don't really need the precise numbers to choose the optimmal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99765839 -19.99663362 -21.99629468]\n",
      " [-13.99765839 -17.99712654 -19.99688008 -19.99691576]\n",
      " [-19.99663362 -19.99688008 -17.99736736 -13.99803444]\n",
      " [-21.99629468 -19.99691576 -13.99803444   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# rendering the value function, in gridworld's shape:\n",
    "print(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
